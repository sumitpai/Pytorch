{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "# Most of the examples are from the documentation but with some additions and comments\n",
    "# https://pytorch.org/tutorials/index.html\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "#check which version you are using.\n",
    "print torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wf dtype is not specified while creating a tensor, it defaults to float32\n",
      "tensor(1.00000e-11 *\n",
      "       [[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  5.0122,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "*** For floating point values, if dtype is not mentioned in output as above, it is float32 by default. \n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print('If dtype is not specified while creating a tensor, it defaults to float32')\n",
    "w = torch.empty((3,3))  # Un-initialized - contains junk values\n",
    "print w\n",
    "print('\\n*** For floating point values, if dtype is not mentioned in output as above, it is float32 by default. ')\n",
    "print w.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a floating point tensor is not of default type(float32), it gets displayed while printing\n",
      "tensor([[ 0.0119,  0.7231,  0.3239],\n",
      "        [ 0.6263,  0.4370,  0.1674],\n",
      "        [ 0.6706,  0.0675,  0.3328]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand((3,3), dtype = torch.float64)   # random values\n",
    "print 'If a floating point tensor is not of default type(float32), it gets displayed while printing'\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0]])\n",
      "*** For non floating point, if dtype is not mentioned in output as above, default is long(int64)\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((3,3), dtype = torch.long)     # Initialized to zeros\n",
    "print x\n",
    "print('*** For non floating point, if dtype is not mentioned in output as above, default is long(int64)')\n",
    "print x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a integer tensor is not of default type(int64), it gets displayed while printing\n",
      "tensor([[-1.7186e+09,  3.2610e+04,  5.2245e+07],\n",
      "        [ 0.0000e+00,  2.5362e+07,  2.1965e+05],\n",
      "        [ 1.3925e+09,  4.0108e+07,  4.8000e+01]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty((3,3), dtype = torch.int32)    # Un-initialized values\n",
    "print 'If a integer tensor is not of default type(int64), it gets displayed while printing'\n",
    "print z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct tensor from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y is a numpy array\n",
      "[[0.5 0.2]\n",
      " [0.7 2. ]]\n",
      "y.dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Construct tensor from data\n",
    "y = np.asarray([[0.5, 0.2],[0.7,2]], dtype=np.float64) # this is float 64\n",
    "print '\\ny is a numpy array'\n",
    "print y\n",
    "print 'y.dtype:', y.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create a tensor w from y directly\n",
      "tensor([[ 0.5000,  0.2000],\n",
      "        [ 0.7000,  2.0000]], dtype=torch.float64)\n",
      "w.dtype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "w = torch.from_numpy(y) \n",
    "print '\\nCreate a tensor w from y directly'\n",
    "print w\n",
    "print 'w.dtype:', w.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create x from y but by changing type\n",
      "tensor([[ 0,  0],\n",
      "        [ 0,  2]], dtype=torch.int32)\n",
      "x.dtype: torch.int32\n"
     ]
    }
   ],
   "source": [
    "x = torch.IntTensor(y) # We converted to Int\n",
    "print '\\nCreate x from y but by changing type'\n",
    "print x\n",
    "print 'x.dtype:', x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a double tensor directly\n",
      "tensor([[ 0.5000,  0.2000],\n",
      "        [ 0.7000,  2.0000]], dtype=torch.float64)\n",
      "x.dtype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "x = torch.DoubleTensor([[0.5, 0.2],[0.7,2]])\n",
    "print '\\nCreating a double tensor directly'\n",
    "print x\n",
    "print 'x.dtype:', x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8265,  0.0990,  0.9314,  0.8799],\n",
      "        [ 0.7774,  0.4609,  0.4119,  0.7068],\n",
      "        [ 0.6018,  0.6430,  0.8177,  0.4626],\n",
      "        [ 0.8928,  0.2464,  0.7398,  0.4523]])\n"
     ]
    }
   ],
   "source": [
    "#Indexing similar to python\n",
    "x = torch.rand((4,4))\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0990,  0.4609,  0.6430,  0.2464])\n"
     ]
    }
   ],
   "source": [
    "print x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4609,  0.4119],\n",
      "        [ 0.6430,  0.8177]])\n"
     ]
    }
   ],
   "source": [
    "print x[1:3,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4523)\n",
      "tensor(0.4523)\n"
     ]
    }
   ],
   "source": [
    "print x[3,3]\n",
    "print x[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6002,  0.3685,  0.3732,  0.8921,  0.9191],\n",
      "        [ 0.1576,  0.0544,  0.6254,  0.5700,  0.6490],\n",
      "        [ 0.1190,  0.6315,  0.7381,  0.2133,  0.7262],\n",
      "        [ 0.1653,  0.2217,  0.1558,  0.3401,  0.4882],\n",
      "        [ 0.7140,  0.7675,  0.1313,  0.3967,  0.3951]])\n",
      "tensor([[ 0.6002,  0.0000,  0.0000,  0.8921,  0.9191],\n",
      "        [ 0.0000,  0.0000,  0.6254,  0.5700,  0.6490],\n",
      "        [ 0.0000,  0.6315,  0.7381,  0.0000,  0.7262],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7140,  0.7675,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Masking \n",
    "x = torch.rand((5,5))\n",
    "print x\n",
    "x[x<0.5] = 0\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8265,  0.0990,  0.9314,  0.8799],\n",
      "        [ 0.7774,  0.4609,  0.4119,  0.7068],\n",
      "        [ 0.6018,  0.6430,  0.8177,  0.4626],\n",
      "        [ 0.8928,  0.2464,  0.7398,  0.4523]])\n",
      "x: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "#Resizing and reshaping tensor\n",
    "print x\n",
    "print 'x:', x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.8265,  0.0990],\n",
      "          [ 0.9314,  0.8799]],\n",
      "\n",
      "         [[ 0.7774,  0.4609],\n",
      "          [ 0.4119,  0.7068]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6018,  0.6430],\n",
      "          [ 0.8177,  0.4626]],\n",
      "\n",
      "         [[ 0.8928,  0.2464],\n",
      "          [ 0.7398,  0.4523]]]])\n",
      "y: torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.view((2,2,2,2))\n",
    "print y\n",
    "print 'y:', y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8265,  0.0990,  0.9314,  0.8799,  0.7774,  0.4609,  0.4119,\n",
      "          0.7068],\n",
      "        [ 0.6018,  0.6430,  0.8177,  0.4626,  0.8928,  0.2464,  0.7398,\n",
      "          0.4523]])\n",
      "z: torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "z = x.view((-1,8))\n",
    "print z\n",
    "print 'z:', z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***same shapes are always broadcastable\n",
      "('x.size(): ', torch.Size([2, 4, 3]))\n",
      "('y.size(): ', torch.Size([2, 4, 3]))\n",
      "('z.size(): ', torch.Size([2, 4, 3]))\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[ 0.2801,  0.6681,  0.8727],\n",
      "         [ 0.8908,  0.3821,  0.0236],\n",
      "         [ 0.4907,  0.0632,  0.3685],\n",
      "         [ 0.4588,  0.6855,  0.1147]],\n",
      "\n",
      "        [[ 0.4200,  0.2966,  0.4891],\n",
      "         [ 0.9992,  0.1163,  0.0473],\n",
      "         [ 0.0871,  0.4790,  0.4528],\n",
      "         [ 0.5797,  0.3806,  0.8701]]])\n",
      "tensor([[[ 0.8422,  0.6807,  0.3745],\n",
      "         [ 0.8817,  0.4352,  0.3723],\n",
      "         [ 0.5443,  0.1888,  0.8803],\n",
      "         [ 0.7200,  0.1064,  0.7601]],\n",
      "\n",
      "        [[ 0.1864,  0.3114,  0.2718],\n",
      "         [ 0.5344,  0.6641,  0.5338],\n",
      "         [ 0.1954,  0.0441,  0.1457],\n",
      "         [ 0.5033,  0.3010,  0.5601]]])\n",
      "tensor([[[ 1.1223,  1.3488,  1.2472],\n",
      "         [ 1.7725,  0.8173,  0.3959],\n",
      "         [ 1.0349,  0.2520,  1.2488],\n",
      "         [ 1.1788,  0.7919,  0.8748]],\n",
      "\n",
      "        [[ 0.6064,  0.6080,  0.7609],\n",
      "         [ 1.5336,  0.7805,  0.5811],\n",
      "         [ 0.2825,  0.5231,  0.5985],\n",
      "         [ 1.0830,  0.6815,  1.4302]]])\n"
     ]
    }
   ],
   "source": [
    "# Two tensors are “broadcastable” if the following rules hold:\n",
    "#       1. Each tensor has at least one dimension.\n",
    "#       2. When iterating over the dimension sizes, starting at the trailing dimension, \n",
    "#           the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "x = torch.rand(2,4,3)\n",
    "y = torch.rand(2,4,3)\n",
    "z = x + y\n",
    "print '***same shapes are always broadcastable'\n",
    "print('x.size(): ', x.size())\n",
    "print('y.size(): ', y.size())\n",
    "print('z.size(): ', z.size())\n",
    "print '\\n\\n'\n",
    "print x\n",
    "print y\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***x and y are not broadcastable, because x does not have at least 1 dimension\n",
      "('x.size(): ', torch.Size([0]))\n",
      "('y.size(): ', torch.Size([2, 2]))\n",
      "tensor([])\n",
      "tensor(1.00000e-23 *\n",
      "       [[-1.4866,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "*** TODO: uncomment the lines and you will get error\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty((0,))\n",
    "y = torch.empty(2,2)\n",
    "\n",
    "print '***x and y are not broadcastable, because x does not have at least 1 dimension'\n",
    "print('x.size(): ', x.size())\n",
    "print('y.size(): ', y.size())\n",
    "\n",
    "print x\n",
    "print y\n",
    "\n",
    "# z = x + y\n",
    "# print('z.size(): ', z.size())\n",
    "# print '\\n\\n'\n",
    "# print z\n",
    "print '*** TODO: uncomment the lines and you will get error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** x and y are broadcastable:\n",
      "       1st trailing dimension: x size == y size\n",
      "       2nd trailing dimension: x has size 1\n",
      "       3rd trailing dimension: x size == y size\n",
      "       4th trailing dimension: x dimension doesn't exist\n",
      "\n",
      "\n",
      "('x.size(): ', torch.Size([3, 1, 2]))\n",
      "('y.size(): ', torch.Size([2, 3, 4, 2]))\n",
      "('z.size(): ', torch.Size([2, 3, 4, 2]))\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[ 0.5311,  0.7328]],\n",
      "\n",
      "        [[ 0.8943,  0.6699]],\n",
      "\n",
      "        [[ 0.2537,  0.5200]]])\n",
      "tensor([[[[ 0.0524,  0.5522],\n",
      "          [ 0.2590,  0.9951],\n",
      "          [ 0.8173,  0.7871],\n",
      "          [ 0.3239,  0.7897]],\n",
      "\n",
      "         [[ 0.8344,  0.2681],\n",
      "          [ 0.4990,  0.9582],\n",
      "          [ 0.4239,  0.5703],\n",
      "          [ 0.9591,  0.1306]],\n",
      "\n",
      "         [[ 0.2504,  0.4479],\n",
      "          [ 0.1704,  0.3618],\n",
      "          [ 0.6148,  0.5104],\n",
      "          [ 0.1540,  0.0082]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7353,  0.3530],\n",
      "          [ 0.6292,  0.9270],\n",
      "          [ 0.2401,  0.5074],\n",
      "          [ 0.1833,  0.9220]],\n",
      "\n",
      "         [[ 0.1890,  0.4441],\n",
      "          [ 0.4173,  0.4655],\n",
      "          [ 0.1990,  0.3232],\n",
      "          [ 0.8795,  0.7190]],\n",
      "\n",
      "         [[ 0.0773,  0.2080],\n",
      "          [ 0.5784,  0.9240],\n",
      "          [ 0.7406,  0.5971],\n",
      "          [ 0.0488,  0.8418]]]])\n",
      "tensor([[[[ 0.5835,  1.2850],\n",
      "          [ 0.7901,  1.7279],\n",
      "          [ 1.3484,  1.5200],\n",
      "          [ 0.8550,  1.5225]],\n",
      "\n",
      "         [[ 1.7287,  0.9380],\n",
      "          [ 1.3933,  1.6281],\n",
      "          [ 1.3182,  1.2402],\n",
      "          [ 1.8534,  0.8005]],\n",
      "\n",
      "         [[ 0.5042,  0.9679],\n",
      "          [ 0.4242,  0.8818],\n",
      "          [ 0.8685,  1.0303],\n",
      "          [ 0.4077,  0.5282]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2664,  1.0858],\n",
      "          [ 1.1603,  1.6598],\n",
      "          [ 0.7712,  1.2402],\n",
      "          [ 0.7144,  1.6548]],\n",
      "\n",
      "         [[ 1.0834,  1.1140],\n",
      "          [ 1.3116,  1.1355],\n",
      "          [ 1.0934,  0.9932],\n",
      "          [ 1.7738,  1.3889]],\n",
      "\n",
      "         [[ 0.3310,  0.7280],\n",
      "          [ 0.8322,  1.4440],\n",
      "          [ 0.9944,  1.1171],\n",
      "          [ 0.3026,  1.3618]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(  3,1,2)\n",
    "y = torch.rand(2,3,4,2)\n",
    "z = x + y\n",
    "print '*** x and y are broadcastable:'\n",
    "print '       1st trailing dimension: x size == y size'\n",
    "print '       2nd trailing dimension: x has size 1'\n",
    "print '       3rd trailing dimension: x size == y size'\n",
    "print \"       4th trailing dimension: x dimension doesn't exist\"\n",
    "print '\\n'\n",
    "print('x.size(): ', x.size())\n",
    "print('y.size(): ', y.size())\n",
    "print('z.size(): ', z.size())\n",
    "print '\\n\\n'\n",
    "print x\n",
    "print y\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n",
      "('x.size(): ', torch.Size([2, 3, 4, 1]))\n",
      "('y.size(): ', torch.Size([2, 1, 1]))\n",
      "tensor([[[[ 0.5055],\n",
      "          [ 0.5379],\n",
      "          [ 0.6831],\n",
      "          [ 0.6969]],\n",
      "\n",
      "         [[ 0.7824],\n",
      "          [ 0.9815],\n",
      "          [ 0.6182],\n",
      "          [ 0.0316]],\n",
      "\n",
      "         [[ 0.7264],\n",
      "          [ 0.2372],\n",
      "          [ 0.4440],\n",
      "          [ 0.0770]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3077],\n",
      "          [ 0.2849],\n",
      "          [ 0.7747],\n",
      "          [ 0.1856]],\n",
      "\n",
      "         [[ 0.0874],\n",
      "          [ 0.2421],\n",
      "          [ 0.9956],\n",
      "          [ 0.9471]],\n",
      "\n",
      "         [[ 0.5775],\n",
      "          [ 0.4440],\n",
      "          [ 0.6023],\n",
      "          [ 0.4377]]]])\n",
      "tensor([[[ 0.2525]],\n",
      "\n",
      "        [[ 0.8433]]])\n",
      "*** TODO: uncomment the lines and you will get error\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(2,3,4,1)\n",
    "y=torch.rand(  2,1,1)\n",
    "print 'x and y are not broadcastable, because in the 3rd trailing dimension 3 != 2'\n",
    "print('x.size(): ', x.size())\n",
    "print('y.size(): ', y.size())\n",
    "\n",
    "print x\n",
    "print y\n",
    "\n",
    "# z = x + y\n",
    "# print('z.size(): ', z.size())\n",
    "# print '\\n\\n'\n",
    "# print z\n",
    "print '*** TODO: uncomment the lines and you will get error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inplace operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84665304 0.95385044 0.71751871]\n",
      " [0.54830367 0.50653871 0.99209744]\n",
      " [0.85470537 0.08201796 0.0326165 ]] float64\n",
      "\n",
      "In place addition to torch tensor\n",
      "[[1.84665304 1.95385044 1.71751871]\n",
      " [1.54830367 1.50653871 1.99209744]\n",
      " [1.85470537 1.08201796 1.0326165 ]]\n",
      "\n",
      "Normal addition\n",
      "[[1.84665304 1.95385044 1.71751871]\n",
      " [1.54830367 1.50653871 1.99209744]\n",
      " [1.85470537 1.08201796 1.0326165 ]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor to numpy related details\n",
    "x = torch.rand((3,3), dtype = torch.float64)\n",
    "x_np = x.numpy() \n",
    "print x_np, x_np.dtype\n",
    "\n",
    "print '\\nIn place addition to torch tensor'\n",
    "x.add_(1) # if x is modified in place, numpy array also changes as it points to same location\n",
    "print x_np\n",
    "\n",
    "print '\\nNormal addition'\n",
    "x = x + 1 # if x is NOT modified in place, numpy array doesnt change\n",
    "print x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]]\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1.]], dtype=torch.float64)\n",
      "\n",
      "In place addition to numpy array\n",
      "[[2. 2. 2. 2. 2.]]\n",
      "tensor([[ 2.,  2.,  2.,  2.,  2.]], dtype=torch.float64)\n",
      "\n",
      "Normal addition\n",
      "[[3. 3. 3. 3. 3.]]\n",
      "tensor([[ 2.,  2.,  2.,  2.,  2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Numpy to tensor related details\n",
    "import numpy as np\n",
    "a = np.ones((1,5), dtype = np.float64)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print '\\nIn place addition to numpy array'\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print '\\nNormal addition'\n",
    "a = a+1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU - GPU Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda support not available\n",
      "\n",
      "\n",
      "***Once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed in on the same device as the tensor.\n",
      "\n",
      "\n",
      "*** Cross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality such as to() and cuda(). \n",
      "Any attempts to launch ops on tensors spread across different devices will raise an error.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object : uses default cuda device\n",
    "    device_gpu0 = torch.device(\"cuda:0\")   # Uses gpu 0(indexed from 0)\n",
    "    print device\n",
    "    x = torch.rand((3,3))     #non cuda tensor\n",
    "    x = x.to(device)          #converts non cuda to cuda and loads on default gpu\n",
    "    y = torch.tensor([1,2]).cuda() #converts non cuda to cuda and loads on default gpu\n",
    "    \n",
    "    u = torch.ones((3,3), device=device_gpu0) # loads on GPU 0\n",
    "    v = torch.ones((3,3), device=device_gpu0) # loads on GPU 0\n",
    "    \n",
    "    with torch.cuda.device(1): #loads the following lines on GPU 1 \n",
    "        a = torch.tensor([1., 2.]).cuda() # loads on GPU 1 irrespective of default GPU\n",
    "        b = torch.tensor([1., 2.]).cuda() # loads on GPU 1 irrespective of default GPU\n",
    "        c = a + b # stored on GPU 1 irrespective of default GPU\n",
    "        \n",
    "        w = u + v # stored on GPU 0 because u and v belong to gpu 0\n",
    "        z = a + u # Raises error as the two variables are on different GPU\n",
    "else:\n",
    "    print 'Cuda support not available'\n",
    "    \n",
    "print '\\n\\n***Once a tensor is allocated, you can do operations on it irrespective of the selected device, \\\n",
    "and the results will be always placed in on the same device as the tensor.'\n",
    "    \n",
    "print '\\n\\n*** Cross-GPU operations are not allowed by default, with the exception of copy_() \\\n",
    "and other methods with copy-like functionality such as to() and cuda(). '\n",
    "print 'Any attempts to launch ops on tensors spread across different devices will raise an error.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to convert a tensor to cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones((3,3), device=device)\n",
    "    z = y.to('cpu')\n",
    "    x = y.cpu()\n",
    "    # Alternatively\n",
    "    device_cpu = torch.device('cpu')\n",
    "    z = y.to(device_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "('x.requires_grad:', True)\n",
      "('y.requires_grad:', True)\n",
      "('z.requires_grad:', True)\n",
      "('out.requires_grad:', True)\n",
      "('Grad fn of x:', None)\n",
      "('Grad fn of y:', <AddBackward0 object at 0x7f622e5abbd0>)\n",
      "('Grad fn of z:', <MulBackward0 object at 0x7f622e5abd50>)\n",
      "('Grad fn of out:', <MeanBackward1 object at 0x7f622e5abbd0>)\n",
      "\n",
      "\n",
      "Visualize the chain:\n",
      "(<MeanBackward1 object at 0x7f622d6db950>, '-->')\n",
      "(<MulBackward0 object at 0x7f622e649150>, '-->')\n",
      "(<MulBackward1 object at 0x7f622e6491d0>, '-->')\n",
      "(<AddBackward0 object at 0x7f622e649110>, '-->')\n",
      "(<AccumulateGrad object at 0x7f622e649250>, '-->')\n",
      "-------\n",
      "(<AddBackward0 object at 0x7f622e649110>, '-->')\n",
      "(<AccumulateGrad object at 0x7f622e649250>, '-->')\n",
      "-------\n",
      "\n",
      "\n",
      "***TODO: Now set requires_grad of x to False and check what happens\n"
     ]
    }
   ],
   "source": [
    "# Autograd Package\n",
    "# torch.Tensor is the central class of the package. \n",
    "# If you set its attribute .requires_grad as True, it starts to track all operations on it.\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#Tensor and Function are interconnected and build up an acyclic graph, \n",
    "# that encodes a complete history of computation.\n",
    "\n",
    "# Each variable has a .grad_fn attribute that references a Function that has created the Tensor \n",
    "# (except for Tensors created by the user - their grad_fn is None).\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print('x.requires_grad:', x.requires_grad)\n",
    "print('y.requires_grad:', y.requires_grad)\n",
    "print('z.requires_grad:', z.requires_grad)\n",
    "print('out.requires_grad:', out.requires_grad)\n",
    "print('Grad fn of x:', x.grad_fn)\n",
    "print('Grad fn of y:', y.grad_fn)\n",
    "print('Grad fn of z:', z.grad_fn)\n",
    "print('Grad fn of out:', out.grad_fn)\n",
    "\n",
    "\n",
    "\n",
    "def print_gradFn(parent_fn):\n",
    "    print(parent_fn , '-->')\n",
    "    if parent_fn is not None and len(parent_fn.next_functions)>0:\n",
    "        for i in range(len(parent_fn.next_functions)):\n",
    "            print_gradFn(parent_fn.next_functions[i][0])\n",
    "    else:\n",
    "        print('-------')\n",
    "    \n",
    "print('\\n\\nVisualize the chain:')    \n",
    "print_gradFn(out.grad_fn)\n",
    "\n",
    "print('\\n\\n***TODO: Now set requires_grad of x to ' +  str(not x.requires_grad) + ' and check what happens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y.requires_grad:', True)\n",
      "('y_mean1.requires_grad:', False)\n",
      "('y_mean2.requires_grad:', True)\n",
      "('y_mean2_sq.requires_grad:', True)\n"
     ]
    }
   ],
   "source": [
    "# To prevent tracking history (and using memory), you can also wrap the code block in with torch.no_grad():. \n",
    "# This can be particularly helpful when evaluating a model because the model may have trainable parameters \n",
    "# with requires_grad=True, but we don’t need the gradients.\n",
    "print ('y.requires_grad:', y.requires_grad)\n",
    "with torch.no_grad():\n",
    "    y_mean1 = y.mean() # doesnt add to computational graph\n",
    "print ('y_mean1.requires_grad:', y_mean1.requires_grad)    \n",
    "\n",
    "y_mean2 = y.mean() # adds to computational graph\n",
    "y_mean2_sq = y_mean2 * y_mean2 # gradient is tracked\n",
    "print ('y_mean2.requires_grad:', y_mean2.requires_grad)\n",
    "print ('y_mean2_sq.requires_grad:', y_mean2_sq.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y.requires_grad:', True)\n",
      "('y_mean3.requires_grad:', False)\n",
      "('y_mean3_sq.requires_grad:', False)\n"
     ]
    }
   ],
   "source": [
    "# To stop a tensor from tracking history, you can call .detach() to detach it from the computation history, \n",
    "# and to prevent future computation from being tracked.\n",
    "y_mean3 = y.detach().mean() # detaches from computational graph for computation of y_mean3\n",
    "y_mean3_sq = y_mean3 * y_mean3 # gradient is tracked\n",
    "print ('y.requires_grad:', y.requires_grad)\n",
    "print ('y_mean3.requires_grad:', y_mean3.requires_grad)\n",
    "print ('y_mean3_sq.requires_grad:', y_mean3_sq.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x grad:', None)\n",
      "('y grad:', None)\n",
      "('z grad:', None)\n",
      "('out grad:', None)\n",
      "('x.requires_grad:', True)\n",
      "('y.requires_grad:', True)\n",
      "('z.requires_grad:', True)\n",
      "('out.requires_grad:', True)\n",
      "\n",
      "\n",
      "***TODO: Compute the derivative by hand and plug in the value of x and see if it matches\n",
      "\n",
      "('x grad:', tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]]))\n",
      "('y grad:', None)\n",
      "('z grad:', None)\n",
      "('out grad:', None)\n"
     ]
    }
   ],
   "source": [
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "print('x grad:', x.grad)\n",
    "print('y grad:', y.grad)\n",
    "print('z grad:', z.grad)\n",
    "print('out grad:', out.grad)\n",
    "print('x.requires_grad:', x.requires_grad)\n",
    "print('y.requires_grad:', y.requires_grad)\n",
    "print('z.requires_grad:', z.requires_grad)\n",
    "print('out.requires_grad:', out.requires_grad)\n",
    "# If you want to compute the derivatives, you can call .backward() on a Tensor\n",
    "# When you finish your computation you can call .backward() and have all the gradients computed automatically. \n",
    "out.backward()\n",
    "print('\\n\\n***TODO: Compute the derivative by hand and plug in the value of x and see if it matches\\n')\n",
    "print('x grad:', x.grad)\n",
    "print('y grad:', y.grad)\n",
    "print('z grad:', z.grad)\n",
    "print('out grad:', out.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size before squeezing:  torch.Size([3, 1, 4, 5, 1, 6])\n",
      "x.size after squeezing :  torch.Size([3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# squeeze removes all dimensions of size 1\n",
    "x = torch.rand((3,1,4,5,1,6))\n",
    "print 'x.size before squeezing: ', x.size()\n",
    "x = x.squeeze()\n",
    "print 'x.size after squeezing : ', x.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.size:  torch.Size([64, 3, 2])\n",
      "x.size:  torch.Size([128, 3, 2])\n",
      "w,x concatenated along dim 0:  torch.Size([192, 3, 2])\n",
      "***Example could be you have 2 set of feature maps and you want to combine them,then concatinate along the channel dimension\n",
      "*** All other dims must be of same size. The concat dim may have different size\n"
     ]
    }
   ],
   "source": [
    "# torch.cat concatenates two tensors in a given dimension\n",
    "w = torch.rand(64,3,2)\n",
    "x = torch.rand(128,3,2)\n",
    "print 'w.size: ', w.size()\n",
    "print 'x.size: ', x.size()\n",
    "\n",
    "y = torch.cat((w,x), 0)\n",
    "print 'w,x concatenated along dim 0: ', y.size()\n",
    "print '***Example could be you have 2 set of feature maps and you want to combine them,\\\n",
    "then concatinate along the channel dimension'\n",
    "print '*** All other dims must be of same size. The concat dim may have different size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.size:  torch.Size([3, 5, 4])\n",
      "x.size:  torch.Size([3, 5, 4])\n",
      "z.size:  torch.Size([3, 2, 5, 4])\n",
      "***Example could be you have 5 different images and you want to create a batch. then stack along dim 0\n",
      "***All the dims must be of same size.\n",
      "***TODO: compare with torch.cat and see the difference\n"
     ]
    }
   ],
   "source": [
    "# torch.stack concatenates two tensors along a new dimension\n",
    "w = torch.rand(3,5,4)\n",
    "x = torch.rand(3,5,4)\n",
    "print 'w.size: ', w.size()\n",
    "print 'x.size: ', x.size()\n",
    "\n",
    "z = torch.stack((w,x), 1)\n",
    "print 'z.size: ', z.size()\n",
    "print '***Example could be you have 5 different images and you want to create a batch. then stack along dim 0'\n",
    "print '***All the dims must be of same size.'\n",
    "print '***TODO: compare with torch.cat and see the difference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size:  torch.Size([3, 4, 5])\n",
      "y.size:  torch.Size([3, 1, 4, 5])\n",
      "***Example you have gray channel images stacked to form a batch, but we need to add an empty dimension of size 1 as above\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze adds a new dimension at specified dim\n",
    "x = torch.rand(3,4,5)\n",
    "y = torch.unsqueeze(x,1)\n",
    "\n",
    "print 'x.size: ', x.size()\n",
    "print 'y.size: ', y.size()\n",
    "print '***Example you have gray channel images stacked to form a batch, but we need to add an empty dimension of size 1 as above'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  155., -177.,   65.,  -90.,   33.,  125.,  -60.,   35.,\n",
       "          21.])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.where(cond, t1, t2) takes in a condition and returns the values from first tensor if condition is true, \n",
    "# else returns from tensor 2\n",
    "x = torch.FloatTensor([0, 155, 183, 65, 270, 33, 125, 300, 35, 21])\n",
    "torch.where(x > 180, x-360, x)\n",
    "\n",
    "print '***Example you have angles between 0 to 360 and you want to convert into -180 to 180 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0738,  0.1720,  0.9996, -0.9688],\n",
      "        [-0.6914, -0.3050, -0.9588, -2.1365],\n",
      "        [ 0.7953, -0.5974, -1.6095, -0.4417],\n",
      "        [-0.0047, -1.9251, -0.0936,  0.7265]])\n",
      "tensor([ 2,  1,  0,  3])\n",
      "tensor([ 0.9996, -0.3050,  0.7953,  0.7265]) tensor([ 2,  1,  0,  3])\n"
     ]
    }
   ],
   "source": [
    "# torch.argmax and torch.max\n",
    "a = torch.randn(4,4)\n",
    "print a\n",
    "print torch.argmax(a, dim=1)\n",
    "maxVal, maxInd = torch.max(a,dim=1)\n",
    "print maxVal, maxInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
